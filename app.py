import streamlit as st
import openai
import edge_tts
import asyncio
import io
import base64
import re
from streamlit_mic_recorder import mic_recorder

# --- 1. C·∫§U H√åNH TRANG ---
st.set_page_config(page_title="Super Fast Voice Chat", page_icon="‚ö°")
st.title("‚ö° Voice Chat: Streaming Real-time")

# --- 2. SIDEBAR C√ÄI ƒê·∫∂T ---
with st.sidebar:
    st.header("C√†i ƒë·∫∑t")
    api_key = st.text_input("Nh·∫≠p OpenAI API Key:", type="password")
    voice_option = st.selectbox(
        "Ch·ªçn gi·ªçng ƒë·ªçc:",
        ["vi-VN-HoaiMyNeural", "vi-VN-NamMinhNeural"]
    )
    
    st.markdown("---")
    if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat"):
        st.session_state.messages = []
        st.rerun()

# Ki·ªÉm tra API Key
if not api_key:
    st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p OpenAI API Key ·ªü thanh b√™n tr√°i ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
    st.stop()

client = openai.OpenAI(api_key=api_key)

# Kh·ªüi t·∫°o l·ªãch s·ª≠ chat
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- 3. JAVASCRIPT PLAYER (TR√ÅI TIM C·ª¶A H·ªÜ TH·ªêNG) ---
# T·∫°o m·ªôt tr√¨nh ph√°t √¢m thanh ·∫©n, t·ª± ƒë·ªông x·∫øp h√†ng c√°c ƒëo·∫°n audio ƒë∆∞·ª£c g·ª≠i xu·ªëng
def setup_audio_player():
    js_code = """
        <script>
            // H√†ng ƒë·ª£i √¢m thanh
            window.audioQueue = [];
            window.isPlaying = false;

            // H√†m l·∫•y audio t·ª´ h√†ng ƒë·ª£i v√† ph√°t
            async function playNext() {
                if (window.audioQueue.length === 0) {
                    window.isPlaying = false;
                    return;
                }
                window.isPlaying = true;
                const audioData = window.audioQueue.shift();
                const audio = new Audio("data:audio/mp3;base64," + audioData);
                
                // Khi ƒëo·∫°n n√†y h·∫øt, t·ª± ƒë·ªông g·ªçi ƒëo·∫°n ti·∫øp theo
                audio.onended = function() {
                    playNext();
                };
                
                try {
                    await audio.play();
                } catch (e) {
                    console.error("Autoplay blocked or error:", e);
                    window.isPlaying = false; 
                }
            }

            // L·∫Øng nghe s·ª± ki·ªán t·ª´ Python g·ª≠i xu·ªëng
            window.parent.document.addEventListener('streamlit:play_chunk', function(e) {
                const b64 = e.detail.base64;
                window.audioQueue.push(b64);
                // N·∫øu ch∆∞a ph√°t g√¨ th√¨ b·∫Øt ƒë·∫ßu ph√°t ngay
                if (!window.isPlaying) {
                    playNext();
                }
            });
        </script>
    """
    # Ch√®n JS v√†o trang (height=0 ƒë·ªÉ ·∫©n)
    st.components.v1.html(js_code, height=0, width=0)

# G·ªçi setup m·ªói l·∫ßn app rerun
setup_audio_player()

# --- 4. C√ÅC H√ÄM X·ª¨ L√ù LOGIC ---

def stream_audio_chunk_to_js(audio_bytes):
    """Chuy·ªÉn bytes √¢m thanh th√†nh Base64 v√† b·∫Øn s·ª± ki·ªán xu·ªëng JS"""
    b64 = base64.b64encode(audio_bytes).decode()
    js_trigger = f"""
        <script>
            var event = new CustomEvent('streamlit:play_chunk', {{ detail: {{ base64: "{b64}" }} }});
            window.parent.document.dispatchEvent(event);
        </script>
    """
    st.components.v1.html(js_trigger, height=0, width=0)

async def generate_audio_chunk(text, voice):
    """T·∫°o audio t·ª´ text b·∫±ng Edge TTS (X·ª≠ l√Ω trong RAM)"""
    if not text or not text.strip():
        return None
    
    communicate = edge_tts.Communicate(text, voice)
    mp3_fp = io.BytesIO()
    
    async for chunk in communicate.stream():
        if chunk["type"] == "audio":
            mp3_fp.write(chunk["data"])
            
    return mp3_fp.getvalue()

def transcribe_audio(audio_bytes):
    """Speech-to-Text d√πng OpenAI Whisper"""
    audio_file = io.BytesIO(audio_bytes)
    audio_file.name = "voice.wav" # Whisper c·∫ßn t√™n file gi·∫£ l·∫≠p
    try:
        transcript = client.audio.transcriptions.create(
            model="whisper-1", 
            file=audio_file, 
            language="vi"
        )
        return transcript.text
    except Exception as e:
        st.error(f"L·ªói STT: {e}")
        return None

# --- 5. GIAO DI·ªÜN & MAIN LOOP ---

# Hi·ªÉn th·ªã n√∫t ghi √¢m
st.markdown("### üéôÔ∏è B·∫•m n√∫t b√™n d∆∞·ªõi ƒë·ªÉ n√≥i chuy·ªán")
col1, col2 = st.columns([1, 5])
with col1:
    audio_input = mic_recorder(
        start_prompt="Ghi √¢m",
        stop_prompt="D·ª´ng & G·ª≠i",
        just_once=True,
        key='recorder'
    )

# X·ª≠ l√Ω khi c√≥ √¢m thanh
if audio_input:
    # A. Transcribe
    user_text = transcribe_audio(audio_input['bytes'])
    
    if user_text:
        # B. Hi·ªÉn th·ªã User Text
        st.session_state.messages.append({"role": "user", "content": user_text})
        with st.chat_message("user"):
            st.write(user_text)
        
        # C. X·ª≠ l√Ω AI & Streaming
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            current_sentence = ""
            
            # G·ªçi GPT-4o-mini v·ªõi ch·∫ø ƒë·ªô stream
            stream = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "B·∫°n l√† tr·ª£ l√Ω AI, tr·∫£ l·ªùi ng·∫Øn g·ªçn, t·ª± nhi√™n, th√¢n thi·ªán."},
                    *st.session_state.messages
                ],
                stream=True,
            )
            
            # V√≤ng l·∫∑p x·ª≠ l√Ω t·ª´ng token (ch·ªØ) AI sinh ra
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    token = chunk.choices[0].delta.content
                    full_response += token
                    current_sentence += token
                    
                    # C·∫≠p nh·∫≠t text tr√™n m√†n h√¨nh
                    message_placeholder.markdown(full_response + "‚ñå")
                    
                    # Ki·ªÉm tra d·∫•u c√¢u ƒë·ªÉ ng·∫Øt c√¢u (. ! ? ho·∫∑c xu·ªëng d√≤ng)
                    if re.search(r'[.!?\n]', token):
                        # D·ªçn d·∫πp c√¢u (x√≥a kho·∫£ng tr·∫Øng th·ª´a)
                        text_to_speak = current_sentence.strip()
                        if text_to_speak:
                            # T·∫°o audio cho c√¢u n√†y
                            audio_chunk = asyncio.run(generate_audio_chunk(text_to_speak, voice_option))
                            if audio_chunk:
                                # ƒê·∫©y xu·ªëng h√†ng ƒë·ª£i ph√°t nh·∫°c
                                stream_audio_chunk_to_js(audio_chunk)
                        
                        # Reset c√¢u hi·ªán t·∫°i
                        current_sentence = ""

            # X·ª≠ l√Ω ph·∫ßn d∆∞ c√≤n l·∫°i (n·∫øu AI kh√¥ng k·∫øt th√∫c b·∫±ng d·∫•u c√¢u)
            if current_sentence.strip():
                audio_chunk = asyncio.run(generate_audio_chunk(current_sentence, voice_option))
                if audio_chunk:
                    stream_audio_chunk_to_js(audio_chunk)
            
            # Hi·ªÉn th·ªã b·∫£n ch·ªët cu·ªëi c√πng
            message_placeholder.markdown(full_response)
            st.session_state.messages.append({"role": "assistant", "content": full_response})

# Hi·ªÉn th·ªã l·ªãch s·ª≠ chat b√™n d∆∞·ªõi (tr·ª´ tin nh·∫Øn m·ªõi nh·∫•t ƒë√£ hi·ªán ·ªü tr√™n)
if len(st.session_state.messages) > 2:
    st.markdown("---")
    st.caption("L·ªãch s·ª≠ h·ªôi tho·∫°i c≈©:")
    for msg in st.session_state.messages[:-2]:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
